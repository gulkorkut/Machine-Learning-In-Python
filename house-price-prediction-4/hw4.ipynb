{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-11T17:23:05.560292Z","iopub.execute_input":"2023-06-11T17:23:05.560646Z","iopub.status.idle":"2023-06-11T17:23:05.584515Z","shell.execute_reply.started":"2023-06-11T17:23:05.560617Z","shell.execute_reply":"2023-06-11T17:23:05.583155Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/housing-prices-competition-for-kaggle-learn-users/train.csv\n/kaggle/input/housing-prices-competition-for-kaggle-learn-users/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the data\nX_train = pd.read_csv('../input/housing-prices-competition-for-kaggle-learn-users/train.csv', index_col='Id') \nX_test = pd.read_csv('../input/housing-prices-competition-for-kaggle-learn-users/test.csv', index_col='Id')\n\nprint(X_train.shape)\nprint(X_test.shape)\n\n#Remove rows with missing target, separate target from predictors\nX_train.dropna(axis=0, subset=['SalePrice'], inplace=True)\nprint(X_train.shape)\n\n#Separate target from predictors (i.e. input features)\ny_train = X_train.SalePrice\nX_train.drop(['SalePrice'], axis=1, inplace=True)\n\n#Perform ordinal encoding to categorical feature columns \nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Get the columns that contain strings and treat them as categorical\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_test[col]).issubset(set(X_train[col]))]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n\n# Drop categorical columns that will not be encoded\nX_train_ordinal = X_train.drop(bad_label_cols, axis=1)\nX_test_ordinal = X_test.drop(bad_label_cols, axis=1)\n\n# Apply ordinal encoder \nordinal_encoder = OrdinalEncoder() # Your code here\nX_train_ordinal[good_label_cols] = ordinal_encoder.fit_transform(X_train_ordinal[good_label_cols])\nX_test_ordinal[good_label_cols] = ordinal_encoder.transform(X_test_ordinal[good_label_cols])\n\n#fill missing values in train and test sets using IterativeImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer \n\nfinal_imputer = IterativeImputer(max_iter=3, random_state=0) \nX_train_imputed = final_imputer.fit_transform(X_train_ordinal)\nX_test_imputed = final_imputer.transform(X_test_ordinal)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:23:05.586910Z","iopub.execute_input":"2023-06-11T17:23:05.587636Z","iopub.status.idle":"2023-06-11T17:23:09.353722Z","shell.execute_reply.started":"2023-06-11T17:23:05.587597Z","shell.execute_reply":"2023-06-11T17:23:09.352161Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(1460, 80)\n(1459, 79)\n(1460, 80)\nCategorical columns that will be ordinal encoded: ['Street', 'Alley', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition']\n\nCategorical columns that will be dropped from the dataset: ['KitchenQual', 'Exterior2nd', 'Functional', 'SaleType', 'Exterior1st', 'MSZoning', 'Utilities']\n","output_type":"stream"}]},{"cell_type":"code","source":"#Question 1a\nimport xgboost as xgb\n\nmodel = xgb.XGBRegressor(n_jobs=-1, random_state=0)\n\nprint(model.n_estimators)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:23:09.356356Z","iopub.execute_input":"2023-06-11T17:23:09.356892Z","iopub.status.idle":"2023-06-11T17:23:09.425037Z","shell.execute_reply.started":"2023-06-11T17:23:09.356846Z","shell.execute_reply":"2023-06-11T17:23:09.423601Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"100\n","output_type":"stream"}]},{"cell_type":"markdown","source":"default value of n_estimators=100      default value of learning_rate=0.1","metadata":{}},{"cell_type":"code","source":"#Question 1b\n\nimport xgboost as xgb\nimport pandas as pd\n\nmodel = xgb.XGBRegressor(n_jobs=-1, random_state=0)\n\nmodel.fit(X_train_imputed, y_train)\n\npredictions = model.predict(X_test_imputed)\n\nsubmission = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\n\nsubmission.to_csv('submission_default.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:23:09.429288Z","iopub.execute_input":"2023-06-11T17:23:09.431425Z","iopub.status.idle":"2023-06-11T17:23:09.944697Z","shell.execute_reply.started":"2023-06-11T17:23:09.431366Z","shell.execute_reply":"2023-06-11T17:23:09.943846Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Score: 16346.66286","metadata":{}},{"cell_type":"code","source":"#Question 2\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nn_estimators_values = [100, 500, 1000]\nlearning_rate_values = [0.01, 0.05, 0.1]\n\nparameters = {'n_estimators': n_estimators_values, 'learning_rate': learning_rate_values}\n\nmodel = xgb.XGBRegressor(n_jobs=-1, random_state=0)\n\nopt = GridSearchCV(model, parameters)\n\nopt.fit(X_train_imputed, y_train)\n\nprint(\"Optimum n_estimators:\", opt.best_params_['n_estimators'])\nprint(\"Optimum learning_rate:\", opt.best_params_['learning_rate'])\n\npredictions = opt.predict(X_test_imputed)\n\nsubmission = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\n\nsubmission.to_csv('submission_grid_search.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:23:09.945956Z","iopub.execute_input":"2023-06-11T17:23:09.946475Z","iopub.status.idle":"2023-06-11T17:24:52.609720Z","shell.execute_reply.started":"2023-06-11T17:23:09.946445Z","shell.execute_reply":"2023-06-11T17:24:52.608769Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Optimum n_estimators: 1000\nOptimum learning_rate: 0.01\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Optimum n_estimators: 1000\nOptimum learning_rate: 0.01","metadata":{}},{"cell_type":"markdown","source":"Score: 14916.67171","metadata":{}},{"cell_type":"code","source":"#Question 3\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, loguniform\n\ndistributions = {\n    'n_estimators': randint(100, 1001),\n    'learning_rate': loguniform(0.01, 0.1)\n}\n\nmodel = xgb.XGBRegressor(n_jobs=-1, random_state=0)\n\nopt_rand = RandomizedSearchCV(model, distributions, n_iter=9, n_jobs=-1, random_state=0)\n\nopt_rand.fit(X_train_imputed, y_train)\n\nprint(\"Optimum n_estimators:\", opt_rand.best_params_['n_estimators'])\nprint(\"Optimum learning_rate:\", opt_rand.best_params_['learning_rate'])\n\npredictions = opt_rand.predict(X_test_imputed)\n\nsubmission = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\n\nsubmission.to_csv('submission_randomized_search.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:24:52.613541Z","iopub.execute_input":"2023-06-11T17:24:52.615180Z","iopub.status.idle":"2023-06-11T17:26:18.362275Z","shell.execute_reply.started":"2023-06-11T17:24:52.615147Z","shell.execute_reply":"2023-06-11T17:26:18.361143Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Optimum n_estimators: 586\nOptimum learning_rate: 0.024179177243329457\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Optimum n_estimators: 586\nOptimum learning_rate: 0.024179177243329457","metadata":{}},{"cell_type":"markdown","source":"Score: 15011.71971","metadata":{}},{"cell_type":"code","source":"#Question 4\nimport xgboost as xgb\nimport pandas as pd\n\nmodel = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=-1, random_state=0)\n\nmodel.fit(X_train_imputed, y_train)\n\npredictions = model.predict(X_test_imputed)\n\nsubmission = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\n\nsubmission.to_csv('submission_selected.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:26:18.366422Z","iopub.execute_input":"2023-06-11T17:26:18.366791Z","iopub.status.idle":"2023-06-11T17:26:23.089053Z","shell.execute_reply.started":"2023-06-11T17:26:18.366754Z","shell.execute_reply":"2023-06-11T17:26:23.088188Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Score: 14615.44802","metadata":{}},{"cell_type":"code","source":"#Question 5\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\n\ndef score_dataset(X_train_set, y_train_set, n_estimators, learning_rate):\n    \n    model = xgb.XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, n_jobs=-1, random_state=0)\n    \n    scores = -1 * cross_val_score(model, X_train_set, y_train_set, cv=5, scoring='neg_mean_absolute_error')\n    average_mae = scores.mean()\n    \n    return average_mae","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:26:23.093333Z","iopub.execute_input":"2023-06-11T17:26:23.093955Z","iopub.status.idle":"2023-06-11T17:26:23.101598Z","shell.execute_reply.started":"2023-06-11T17:26:23.093923Z","shell.execute_reply":"2023-06-11T17:26:23.100643Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Question 6\ndefault_mae = score_dataset(X_train_imputed, y_train, 100, 0.1)  # q1\ngrid_search_mae = score_dataset(X_train_imputed, y_train, opt.best_params_['n_estimators'], opt.best_params_['learning_rate'])  # q2\nrandomized_search_mae = score_dataset(X_train_imputed, y_train, opt_rand.best_params_['n_estimators'], opt_rand.best_params_['learning_rate'])  # q3\nselected_mae = score_dataset(X_train_imputed, y_train, 1000, 0.05)  # q4\n\nprint(\"Average MAE with default hyper-parameters:\", default_mae)\nprint(\"Average MAE with grid search hyper-parameters:\", grid_search_mae)\nprint(\"Average MAE with randomized search hyper-parameters:\", randomized_search_mae)\nprint(\"Average MAE with selected hyper-parameters:\", selected_mae)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T17:26:23.102538Z","iopub.execute_input":"2023-06-11T17:26:23.102802Z","iopub.status.idle":"2023-06-11T17:27:17.956820Z","shell.execute_reply.started":"2023-06-11T17:26:23.102778Z","shell.execute_reply":"2023-06-11T17:27:17.955979Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Average MAE with default hyper-parameters: 16590.95635166952\nAverage MAE with grid search hyper-parameters: 16130.95675032106\nAverage MAE with randomized search hyper-parameters: 16162.390344071062\nAverage MAE with selected hyper-parameters: 16379.611817744008\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Average MAE with default hyper-parameters: 16590.95635166952\nAverage MAE with grid search hyper-parameters: 16162.390344071062\nAverage MAE with randomized search hyper-parameters: 16162.390344071062\nAverage MAE with selected hyper-parameters: 16379.611817744008","metadata":{}}]}